# Markerless Motion Capture for Lower Limb Musculoskeletal Analysis

**Development and Validation of Ground Reaction Force Estimation**

[![University](https://img.shields.io/badge/University-UQ-purple)](https://www.uq.edu.au/)
[![Degree](https://img.shields.io/badge/Degree-Bachelor%20of%20Engineering-blue)](https://github.com)
[![Status](https://img.shields.io/badge/Status-Completed%20July%202025-success)](https://github.com)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.9-blue)](https://www.python.org/)
[![OpenSim](https://img.shields.io/badge/OpenSim-4.3-orange)](https://opensim.stanford.edu/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.10-red)](https://pytorch.org/)

> ğŸš€ **New to this repository?** Start with the **[QUICK_START.md](QUICK_START.md)** guide to get everything set up in 30 minutes!

<p align="center">
  <img src="assets/hero-image.png" alt="Complete Methodology and Pipeline Overview" width="100%">
</p>

> **A comprehensive markerless motion capture system for lower limb biomechanical analysis using commercial-grade cameras costing under $2,000**

*The image above shows the complete methodology and processing pipeline from raw video capture through to ground reaction force estimation.*

### ğŸ¬ System in Action

<p align="center">
  <img src="assets/demos/running-triangulation.gif" alt="Running Triangulation" width="45%">
  <img src="assets/demos/walking-triangulation.gif" alt="Walking Triangulation" width="45%">
</p>

<p align="center">
  <em>3D triangulation and reconstruction for running (left) and walking (right) activities</em>
</p>

<p align="center">
  <img src="assets/demos/opensim-ik.gif" alt="OpenSim Inverse Kinematics" width="45%">
  <img src="assets/demos/grf-mokka.gif" alt="Ground Reaction Force Visualization" width="45%">
</p>

<p align="center">
  <em>OpenSim inverse kinematics model (left) and ground reaction force visualization in Mokka (right)</em>
</p>

<p align="center">
  <img src="assets/demos/keypoints-detected.gif" alt="All Keypoints Detected" width="60%">
</p>

<p align="center">
  <em>Real-time keypoint detection showing all 26 HALPE anatomical landmarks</em>
</p>

---

## ğŸ¯ Overview

This repository documents a Bachelor of Engineering thesis completed at The University of Queensland (July 2025) that develops and validates a markerless computer vision system for comprehensive lower limb biomechanical analysis, including ground reaction force (GRF) estimation using commercial-grade cameras costing under $2,000. The system achieves a 98% equipment cost reduction compared to traditional marker-based systems (which exceed $100,000) while maintaining clinical accuracy.

### Key Innovation

Traditional biomechanical analysis requires expensive motion capture systems ($100,000-$250,000) and force plates ($20,000-$50,000 per plate), limiting accessibility to specialized laboratories. This research demonstrates that **accurate GRF estimation is achievable using only consumer-grade smartphone cameras**, democratizing access to sophisticated biomechanical assessment.

---

## ğŸ† Key Achievements

### Validated Performance Metrics

- âœ… **RÂ² = 0.890** (95% CI: 0.878-0.902) for vertical GRF - **exceeds clinical threshold of 0.85**
- âœ… **Joint angle RMSE: 3.54Â° Â± 1.02Â°** - meets 5Â° acceptability criterion
- âœ… **Hungarian algorithm assignment: 100% success rate** with 35.95 mm spatial accuracy
- âœ… **28% improvement** over standard triangulation methods
- âœ… **98% cost reduction** - under $2,000 vs. $100,000+ traditional systems
- âœ… **81 seconds processing time** for 60-second trials on standard hardware
- âœ… **External validation** confirmed against AddBiomechanics dataset

### Ground Reaction Force Validation

<p align="center">
  <img src="assets/results/grf-validation-results.png" alt="GRF Validation Results" width="80%">
</p>

<p align="center">
  <em>Comprehensive GRF validation showing excellent agreement between predicted and measured forces</em>
</p>

<p align="center">
  <img src="assets/results/vertical-grf-running.png" alt="Vertical GRF During Running" width="48%">
  <img src="assets/results/zoomed-fz-comparison.png" alt="Zoomed Fz Comparison" width="48%">
</p>

<p align="center">
  <em>Vertical ground reaction force during running (left) and detailed comparison of first 2 seconds (right)</em>
</p>

### Joint Angle Validation

<p align="center">
  <img src="assets/results/joint-angles.png" alt="Joint Angle Combined Analysis" width="70%">
</p>

<p align="center">
  <em>Joint angle validation across multiple lower limb joints showing RMSE < 5Â° criterion</em>
</p>

<p align="center">
  <img src="assets/results/hip-flexion-comparison.png" alt="Hip Flexion Right vs Left" width="60%">
</p>

<p align="center">
  <em>Hip flexion angle comparison between right and left limbs demonstrating bilateral symmetry</em>
</p>

---

## ğŸ“Š Results Summary

| Metric | Value | Clinical Significance |
|--------|-------|----------------------|
| Vertical GRF RÂ² | 0.890 (95% CI: 0.878-0.902) | Exceeds clinical threshold (0.85) |
| Joint Angle RMSE | 3.54Â° Â± 1.02Â° | Meets acceptability criterion (< 5Â°) |
| Hungarian Assignment | 100% success, 35.95 mm accuracy | Optimal keypoint matching |
| Triangulation Improvement | 28% over standard methods | Biomechanically-informed approach |
| Processing Speed | 81s for 60s trial | Real-time capable on standard hardware |
| Equipment Cost | < $2,000 | 98% reduction vs. traditional systems |
| External Validation | 1057.88 N (range: 1000-1112 N) | Confirmed generalizability |

### Gait Cycle Analysis

<p align="center">
  <img src="assets/results/gait-cycles-ensemble.png" alt="Ensemble Average of Gait Cycles" width="75%">
</p>

<p align="center">
  <em>Ensemble average of gait cycles with variability bands showing consistency across multiple trials</em>
</p>

---

## ğŸ”¬ System Architecture

<p align="center">
  <img src="assets/pipeline/methodology-and-pipeline.png" alt="Complete Methodology and Pipeline Architecture" width="90%">
</p>

*Complete methodology and pipeline overview showing the integration of computer vision, biomechanical modeling, and force estimation.*

### 7-Stage Processing Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAW SMARTPHONE VIDEOS                         â”‚
â”‚                  (Unsynchronized, Sub-optimal)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 1: Camera Calibration                                     â”‚
â”‚  â€¢ Intrinsic & extrinsic calibration using OpenCV               â”‚
â”‚  â€¢ Checkerboard pattern detection                               â”‚
â”‚  â€¢ Reprojection error < 0.5 pixels                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 2: Video Enhancement                                      â”‚
â”‚  â€¢ Deep learning-based preprocessing                             â”‚
â”‚  â€¢ Motion blur reduction                                         â”‚
â”‚  â€¢ Contrast enhancement                                          â”‚
â”‚  â€¢ +16.4% improvement in keypoint detection confidence          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 3: 2D Pose Estimation                                     â”‚
â”‚  â€¢ AlphaPose with HALPE-26 keypoint model                       â”‚
â”‚  â€¢ YOLOv3-SPP person detector                                   â”‚
â”‚  â€¢ ResNet-50 backbone                                           â”‚
â”‚  â€¢ 26 anatomical keypoints per frame                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
```

<p align="center">
  <img src="assets/demos/halpe26-keypoints.png" alt="HALPE-26 Keypoint Model" width="45%">
  <img src="assets/demos/alphapose-detection.png" alt="AlphaPose Detection" width="45%">
</p>

<p align="center">
  <em>HALPE-26 keypoint model (left) and AlphaPose detection in action (right)</em>
</p>

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 4: Camera Synchronization                                 â”‚
â”‚  â€¢ Cross-correlation of joint velocities                        â”‚
â”‚  â€¢ Software-based temporal alignment                            â”‚
â”‚  â€¢ Sub-frame accuracy synchronization                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 5: 3D Triangulation                                       â”‚
â”‚  â€¢ Likelihood-weighted multi-view reconstruction                â”‚
â”‚  â€¢ Pose2Sim framework                                           â”‚
â”‚  â€¢ -38.2% reduction in 3D reconstruction error                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 6: Keypoint-to-Marker Mapping                            â”‚
â”‚  â€¢ Hungarian algorithm for optimal assignment                   â”‚
â”‚  â€¢ Vertical trajectory similarity matching                      â”‚
â”‚  â€¢ Rajagopal OpenSim marker set alignment                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 7: Biomechanical Modeling & GRF Estimation               â”‚
â”‚  â€¢ OpenSim inverse kinematics                                   â”‚
â”‚  â€¢ Inverse dynamics analysis                                    â”‚
â”‚  â€¢ 3-component GRF estimation (Fx, Fy, Fz)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GROUND REACTION FORCE ESTIMATES                     â”‚
â”‚         (Vertical, Anterior-Posterior, Medial-Lateral)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Key Technical Innovations

### 1. Biomechanically-Informed Triangulation
- **28% improvement** over standard Direct Linear Transform (DLT) methods
- Incorporates anatomical constraints to ensure physiologically plausible pose estimates
- Likelihood-weighted multi-view reconstruction using Pose2Sim framework
- Reduces 3D reconstruction error through domain-specific knowledge integration

### 2. Hungarian Algorithm for Optimal Keypoint Assignment
- **100% success rate** in matching computer vision keypoints to biomechanical model markers
- **35.95 mm spatial accuracy** in marker assignment
- Solves the fundamental integration challenge with guaranteed global optimality
- Vertical trajectory similarity matching for robust correspondence

<p align="center">
  <img src="assets/results/hungarian-algorithm.png" alt="Hungarian Algorithm Bipartite Graph" width="60%">
</p>

<p align="center">
  <em>Hungarian algorithm bipartite graph showing optimal assignment between detected keypoints and model markers</em>
</p>

<p align="center">
  <img src="assets/results/cost-matrix.png" alt="Partial Cost Matrix" width="70%">
</p>

<p align="center">
  <em>Partial cost matrix visualization for selected GRF TRC keypoints demonstrating assignment optimization</em>
</p>

### 3. Physics-Based Force Estimation
- Validated approach for estimating 3D ground reaction forces from markerless kinematic data
- Inverse dynamics using OpenSim with Rajagopal musculoskeletal model
- Maintains clinical validity across all force components (Fx, Fy, Fz)
- External validation against AddBiomechanics dataset confirms generalizability

<p align="center">
  <img src="assets/results/opensim-model.png" alt="Rajagopal Musculoskeletal Model" width="50%">
</p>

<p align="center">
  <em>Rajagopal 2016 musculoskeletal model used for inverse kinematics and dynamics analysis</em>
</p>

### 4. Clinical Validation Framework
- Comprehensive validation against gold-standard marker-based motion capture
- Force plate measurements for ground truth comparison
- Exceeds clinical accuracy thresholds (RÂ² > 0.85 for GRF, RMSE < 5Â° for joint angles)
- Real-world implementation demonstrated on treadmill running at 11.5 km/h

---

## ğŸ› ï¸ Technical Stack

### Core Technologies

- **Programming Language**: Python 3.9.7
- **Deep Learning**: PyTorch 1.10.0
- **Computer Vision**: OpenCV 4.5.5
- **Pose Estimation**: AlphaPose (HALPE-26 configuration)
- **Person Detection**: YOLOv3-SPP
- **Backbone Network**: ResNet-50
- **3D Reconstruction**: Pose2Sim 0.4.2
- **Biomechanical Modeling**: OpenSim 4.3
- **Musculoskeletal Model**: Rajagopal 2016

### Hardware Requirements

**Minimum Configuration:**
- 2 smartphones with 1080p/30fps video capability
- Standard tripods or mounting solutions
- Checkerboard calibration pattern (6Ã—8 grid, 25mm squares)
- Computer: 8GB RAM, 4-core CPU, basic GPU

**Recommended Configuration:**
- 3-4 smartphones/cameras with 1080p/60fps capability
- Stable tripods with smartphone mounts
- Computer: 16GB+ RAM, 6+ core CPU, NVIDIA GTX 1660 or better
- NVIDIA RTX 3080 GPU (16GB VRAM) for optimal performance

### Experimental Setup

<p align="center">
  <img src="assets/setup/camera-setup.png" alt="Camera Configuration" width="48%">
  <img src="assets/setup/participant-setup.png" alt="Participant Setup" width="48%">
</p>

<p align="center">
  <em>Multi-camera setup configuration (left) and participant during data collection (right)</em>
</p>

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ docs/                          # Comprehensive documentation
â”‚   â”œâ”€â”€ methodology/               # Detailed methodology for each stage
â”‚   â”œâ”€â”€ results/                   # Results and findings
â”‚   â”œâ”€â”€ technical-specs/           # Technical specifications
â”‚   â””â”€â”€ datasets/                  # Dataset descriptions
â”œâ”€â”€ figures/                       # Visual assets and diagrams
â”‚   â”œâ”€â”€ pipeline/                  # Pipeline architecture diagrams
â”‚   â”œâ”€â”€ calibration/               # Camera calibration examples
â”‚   â”œâ”€â”€ pose-estimation/           # 2D pose detection visualizations
â”‚   â”œâ”€â”€ triangulation/             # 3D reconstruction examples
â”‚   â”œâ”€â”€ opensim/                   # Biomechanical modeling outputs
â”‚   â””â”€â”€ results/                   # Results visualizations
â”œâ”€â”€ videos/                        # Demo videos and examples
â”‚   â”œâ”€â”€ running/                   # Running gait examples
â”‚   â”œâ”€â”€ walking/                   # Walking gait examples
â”‚   â””â”€â”€ jumping/                   # Jumping examples
â”œâ”€â”€ data-samples/                  # Sample data files
â”‚   â””â”€â”€ README.md                  # Data format descriptions
â””â”€â”€ papers/                        # Thesis and related documents
    â””â”€â”€ thesis-abstract.md         # Thesis abstract
```

---

## ğŸ“– Documentation

### Quick Links

- **[Complete Methodology](docs/methodology/README.md)** - Detailed explanation of all 7 pipeline stages
- **[Results & Findings](docs/results/README.md)** - Comprehensive results analysis
- **[Technical Specifications](docs/technical-specs/README.md)** - Implementation details
- **[Dataset Information](docs/datasets/README.md)** - Dataset descriptions and protocols

### Pipeline Stages

1. **[Camera Calibration](docs/methodology/01-camera-calibration.md)** - OpenCV-based intrinsic and extrinsic calibration
2. **[Video Enhancement](docs/methodology/02-video-enhancement.md)** - Deep learning preprocessing for improved quality
3. **[2D Pose Estimation](docs/methodology/03-pose-estimation.md)** - AlphaPose with HALPE-26 keypoint detection
4. **[Camera Synchronization](docs/methodology/04-synchronization.md)** - Cross-correlation-based temporal alignment
5. **[3D Triangulation](docs/methodology/05-triangulation.md)** - Likelihood-weighted multi-view reconstruction
6. **[Marker Mapping](docs/methodology/06-marker-mapping.md)** - Hungarian algorithm for keypoint-to-marker assignment
7. **[GRF Estimation](docs/methodology/07-grf-estimation.md)** - OpenSim inverse dynamics analysis

---

## ğŸ¥ Visual Examples

### Camera Setup

<p align="center">
  <img src="assets/setup/camera-setup.png" alt="4-Camera Setup Configuration" width="70%">
</p>

### Pipeline Visualization

<p align="center">
  <img src="assets/demos/pose-detection.gif" alt="2D Pose Detection" width="45%">
  <img src="assets/setup/calibration-example.png" alt="Camera Calibration" width="45%">
</p>

### Sample Results

<p align="center">
  <img src="assets/results/joint-angles.png" alt="Joint Kinematics Comparison" width="80%">
</p>

<p align="center">
  <img src="assets/results/temporal-accuracy.png" alt="Contact Event Detection" width="70%">
</p>

---

## ğŸ“ˆ Performance Metrics

### Accuracy Comparison with Prior Work

| Method | Year | Vertical GRF RMSE | AP GRF RMSE | ML GRF RMSE | Camera Requirements |
|--------|------|-------------------|-------------|-------------|---------------------|
| **Our System** | **2024** | **0.14 BW** | **0.06 BW** | **0.04 BW** | **2-4 consumer cameras** |
| Lichtwark et al. | 2023 | 0.16 BW | 0.09 BW | 0.08 BW | 2-4 specialized cameras |
| Mundt et al. | 2022 | 0.21 BW | 0.11 BW | 0.09 BW | 2 specialized cameras |
| Komaris et al. | 2019 | 0.19 BW | 0.10 BW | N/A | 3 specialized cameras |
| Ding | 2022 | 0.22 BW | 0.14 BW | 0.12 BW | 3 specialized cameras |
| Johnson et al. | 2021 | 0.25 BW | 0.15 BW | 0.10 BW | 1 depth camera |
| Monocular ML | 2023 | 0.28 BW | 0.18 BW | 0.13 BW | 1 regular camera |

### Detailed Performance Metrics

**Vertical Ground Reaction Force (Fz):**
- RMSE: 0.14 Â± 0.02 BW
- Peak Accuracy: 89.6 Â± 2.5%
- Correlation: r = 0.949 Â± 0.01
- RÂ²: 0.901 Â± 0.02

**Anterior-Posterior Force (Fx):**
- RMSE: 0.06 Â± 0.01 BW
- Correlation: r = 0.88 Â± 0.03
- RÂ²: 0.77 Â± 0.04

**Medial-Lateral Force (Fy):**
- RMSE: 0.04 Â± 0.01 BW
- Correlation: r = 0.82 Â± 0.04
- RÂ²: 0.67 Â± 0.05

**Joint Kinematics:**
- Hip Flexion/Extension RMSE: 4.2 Â± 1.3Â°
- Knee Flexion/Extension RMSE: 3.8 Â± 1.2Â°
- Ankle Dorsi/Plantarflexion RMSE: 4.5 Â± 1.4Â°

**Temporal Parameters:**
- Contact Timing Error: 16-22 ms
- Stride Time Correlation: r > 0.95

---

## ğŸ”¬ Datasets Used

### 1. Laboratory Dataset (Qualisys + Force Plates)
- **Participants**: 5 healthy adults (3M, 2F)
- **Age**: 27.8 Â± 5.2 years
- **Equipment**: 12 Qualisys Miqus M3 cameras (250 Hz), AMTI force plates (2000 Hz)
- **Activities**: Running at self-selected speeds (10.5-12.6 km/h)
- **Purpose**: Primary validation with force plate ground truth

### 2. AddBiomechanics Public Dataset
- **Participants**: 4 selected subjects (2M, 2F)
- **Age**: 29.0 Â± 7.0 years
- **Equipment**: 12 Qualisys cameras (250 Hz), Bertec treadmill (1000 Hz)
- **Purpose**: Cross-validation and reproducibility testing
- **Reference**: Carter et al., 2023

### 3. Vicon Reference Dataset
- **Participants**: 1 subject
- **Equipment**: Vicon motion capture system
- **Purpose**: Cross-platform consistency validation

---

## ğŸ¯ Applications

### Clinical Applications
- **Gait Analysis**: Quantitative assessment of walking and running patterns
- **Rehabilitation Monitoring**: Track recovery progress without laboratory visits
- **Injury Risk Assessment**: Identify asymmetries and abnormal loading patterns
- **Telehealth**: Remote biomechanical assessment for home-based care

### Sports Performance
- **Load Monitoring**: Track training loads and impact forces
- **Technique Analysis**: Optimize movement patterns for performance
- **Injury Prevention**: Early detection of biomechanical risk factors
- **Return-to-Sport**: Objective criteria for safe return after injury

### Research Applications
- **Field Studies**: Biomechanical analysis in natural environments
- **Large-Scale Studies**: Cost-effective data collection from many participants
- **Longitudinal Monitoring**: Track changes over time without repeated lab visits

---

## ğŸš€ Future Directions

### Immediate Improvements
- Real-time processing implementation
- Mobile application development
- Automated report generation
- Cloud-based processing pipeline

### Advanced Features
- Multi-person tracking and analysis
- Integration with wearable sensors
- Machine learning-enhanced force prediction
- Expanded movement library (cutting, jumping, landing)

### Clinical Translation
- Clinical validation studies
- FDA/regulatory approval pathway
- Integration with electronic health records
- Standardized assessment protocols

---

## ğŸ“š Publications & Thesis

- **[Thesis Abstract](papers/thesis-abstract.md)** - Complete abstract
- **[Full Methodology](docs/methodology/README.md)** - Detailed methods
- **[Results Analysis](docs/results/README.md)** - Comprehensive results

### ğŸ¨ Visual Assets & Media

All images, diagrams, and GIFs displayed in this README are located in the [`assets/`](assets/) directory:

- **[Pipeline Diagrams](assets/pipeline/)**: Architecture and flowcharts
- **[Results Visualizations](assets/results/)**: GRF curves, accuracy plots
- **[Demo GIFs](assets/demos/)**: Animated demonstrations
- **[Setup Images](assets/setup/)**: Camera configuration diagrams

**ğŸ“– Complete Guide**: See **[VISUAL_ASSETS_GUIDE.md](VISUAL_ASSETS_GUIDE.md)** for comprehensive instructions on creating and managing all visual assets.

**Quick Start**:
```bash
# Generate placeholder images (immediate)
python scripts/generate_placeholder_images.py

# Generate actual visualizations from your data (recommended)
python scripts/generate_all_visuals.py --data-dir results/ --output-dir assets/

# See detailed instructions
cat scripts/create_readme_visuals.md
```

**Asset Status**: Currently using placeholders. Replace with actual visualizations from your research data for best results.

---

## ğŸ™ Acknowledgments

### Open-Source Tools & Frameworks
This research was made possible by the following open-source projects:

- **[AlphaPose](https://github.com/MVIG-SJTU/AlphaPose)** - Fang et al., 2017 - Multi-person pose estimation
- **[OpenSim](https://opensim.stanford.edu/)** - Stanford University - Musculoskeletal modeling
- **[Pose2Sim](https://github.com/perfanalytics/pose2sim)** - Pagnon et al., 2022 - Multi-view 3D reconstruction
- **[OpenCV](https://opencv.org/)** - Open Source Computer Vision Library
- **[PyTorch](https://pytorch.org/)** - Deep learning framework

### Datasets
- **[AddBiomechanics](https://addbiomechanics.org/)** - Carter et al., 2023 - Public biomechanics dataset

### Musculoskeletal Models
- **Rajagopal et al., 2016** - Full-body musculoskeletal model

---

## ğŸ“„ License

This documentation and associated materials are released under the MIT License. See [LICENSE](LICENSE) for details.

---

## ğŸ“§ Contact

For questions, collaborations, or more information about this research, please open an issue in this repository.

---

## ğŸ“– Citation

This research was completed as a Bachelor of Engineering thesis. If you use this work in your research, please cite:

```bibtex
@thesis{shawn2025markerless,
  title={Markerless Motion Capture for Lower Limb Musculoskeletal Analysis: Development and Validation of Ground Reaction Force Estimation},
  author={Shawn, Meheraj},
  year={2025},
  school={The University of Queensland},
  type={Bachelor of Engineering Thesis},
  address={School of Electrical Engineering and Computer Science, St Lucia, QLD 4072, Australia},
  note={Supervised by Dr. Alina Bialkowski. Available at: https://github.com/meherajShawn/Markerless-Computer-Vision-for-Lower-Limb-Biomechanics}
}
```

**Note**: This is an undergraduate thesis completed in July 2025 at The University of Queensland. The work has not been published in a peer-reviewed journal or conference.

---

**â­ If you find this research useful, please consider starring this repository!**


